{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twitter interface for the microtales generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Twitter account https://twitter.com/talesmachine is the interface with the microtales generator. Any user can mention @talesmachine and include a seed, and the generator will tweet the generated microtale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from keras.models import model_from_yaml\n",
    "\n",
    "local_path = \"/home/ubuntu/data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "\n",
    "#Twitter API credentials\n",
    "consumer_key = \"\"\n",
    "consumer_secret = \"\"\n",
    "access_key = \"\"\n",
    "access_secret = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#authorize twitter, initialize tweepy\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_key, access_secret)\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tinys3\n",
    "\n",
    "#AWS credentials, to download the last model from S3\n",
    "aws_access = {}\n",
    "aws_access[\"Key\"] = \"\"\n",
    "aws_access[\"Secret\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Download the model definition and weights from the S3 repository\n",
    "\n",
    "conn = tinys3.Connection(aws_access[\"Key\"], aws_access[\"Secret\"], tls=True)\n",
    "\n",
    "f = conn.get(bucket=\"juandoso-microtales\", key='model_weights.h5')\n",
    "f1 = open(local_path+'model_weights.h5','w')\n",
    "f1.write(f.content)\n",
    "f1.close()\n",
    "\n",
    "f = conn.get(bucket=\"juandoso-microtales\", key='model_definition.yaml')\n",
    "f1 = open(local_path+'model_definition.yaml','w')\n",
    "f1.write(f.content)\n",
    "f1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load the trained model\n",
    "yaml_file = open(local_path+'model_definition.yaml', 'r').read()\n",
    "model = model_from_yaml(yaml_file)\n",
    "\n",
    "model.load_weights(local_path+'model_weights.h5')\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "\n",
    "# the model will be used as a global constant\n",
    "MODEL = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we limit ourselves to the following chars.\n",
    "# Uppercase letters will be represented by prefixing them with a U\n",
    "# - a trick proposed by Zygmunt Zajac http://fastml.com/one-weird-trick-for-training-char-rnns/\n",
    "chars = u'\\n !\"#$%&\\'()*+,-./0123456789:;<=>?@[\\\\]abcdefghijklmnopqrstuvwxyzU' + u'\\xa0\\xbb\\xbf\\xc1\\xc9\\xcd\\xd1\\xd3\\xda\\xe0\\xe1\\xe9\\xed\\xf1\\xf3\\xfa\\xfc\\u2015'\n",
    "charset = set(chars)\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "def fix_char(c):\n",
    "    if c.isupper():\n",
    "        return 'U' + c.lower()\n",
    "    elif c in charset:\n",
    "        return c\n",
    "    elif c in [u'\\xa1', u'\\xb0', u'\\u201c', u'\\u201d']:\n",
    "        return '\"'\n",
    "    elif c in [u'\\u2013', u'\\u2014', u'\\u2212', u'\\u2500']:\n",
    "        return '-'\n",
    "    elif c == u'\\u2026':\n",
    "        return '...'\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "def encode(text):\n",
    "    return ''.join(fix_char(c) for c in text)\n",
    "\n",
    "def decode(chars):\n",
    "    upper = False\n",
    "    for c in chars:\n",
    "        if c == '#': continue\n",
    "        if c == 'U':\n",
    "            upper = True\n",
    "        elif upper:\n",
    "            upper = False\n",
    "            yield c.upper()\n",
    "        else:\n",
    "            yield c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# helper function to sample an index from a probability array\n",
    "def sample(a, temperature=1.0):\n",
    "    a = np.log(a) / temperature\n",
    "    a = np.exp(a) / np.sum(np.exp(a))\n",
    "    # np.random.multinomial throws an error if the probabilities\n",
    "    # sum to > 1 - which they do due to finite precision\n",
    "    while sum(a) > 1:\n",
    "        a /= 1.000001\n",
    "    return np.argmax(np.random.multinomial(1, a, 1))\n",
    "\n",
    "# function to yield generated characters\n",
    "def generate(model, seed, diversity):\n",
    "    _, maxlen, _ = model.input_shape\n",
    "    rseed = seed.rjust(maxlen)\n",
    "    sentence = rseed[-maxlen:]\n",
    "    while True:\n",
    "        x = np.zeros((1, maxlen, len(chars)))\n",
    "        for t, char in enumerate(sentence):\n",
    "            x[0, t, char_indices[char]] = 1.\n",
    "\n",
    "        preds = model.predict(x, verbose=0)[0]\n",
    "        next_index = sample(preds, diversity)\n",
    "        next_char = indices_char[next_index]\n",
    "        yield next_char\n",
    "        sentence = sentence[1:] + next_char\n",
    "\n",
    "# generate n characters from seed\n",
    "def generate_tale(model, seed, diversity, n):\n",
    "    generator = decode(generate(model, seed, diversity))\n",
    "\n",
    "    full_text = []\n",
    "    for _ in range(n):\n",
    "        next_char = generator.next()\n",
    "        full_text.append(next_char)\n",
    "\n",
    "    return ''.join(full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TEMPERATURE = 0.75\n",
    "\n",
    "# generate a new tale for a mention\n",
    "def tale_to_tweet(mention):\n",
    "    author = mention[0]\n",
    "    seed = mention[1].replace(\"@talesmachine \", \"\")\n",
    "    dseed = encode(seed)\n",
    "    t = generate_tale(MODEL, dseed, TEMPERATURE, 139)\n",
    "    return author, seed, t\n",
    "\n",
    "# obtains the last mentions\n",
    "def new_mentions(db_mentions):\n",
    "    mentions = api.mentions_timeline()\n",
    "    new_mentions = []\n",
    "    for m in mentions:\n",
    "        if (m.author.screen_name, m.text, m.id) in dic_mentions:\n",
    "            continue\n",
    "        else:\n",
    "            db_mentions.append((m.author.screen_name, m.text, m.id))\n",
    "            new_mentions.append((m.author.screen_name, m.text, m.id))\n",
    "    \n",
    "    return new_mentions\n",
    "\n",
    "# create a list of responses for the new mentions\n",
    "def new_tales(new_mentions):\n",
    "    new_tales = []\n",
    "    for m in new_mentions:\n",
    "        new_tales.append(tale_to_tweet(m))\n",
    "    return new_tales\n",
    "\n",
    "# tweet the generated tales\n",
    "def respond_with_mentions(new_tales):\n",
    "    if new_tales:\n",
    "        print(\"new tale:\", new_tales)\n",
    "        for t in new_tales:\n",
    "            try:\n",
    "                tweet = \"Este cuento esta dedicado a @%s. %s ...\" % (t[0], t[1])\n",
    "                api.update_status(tweet)\n",
    "                api.update_status(t[2])\n",
    "            except Exception as e:\n",
    "                print(\"Error at tweeting:\", e)\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get the current mentions\n",
    "mentions = api.mentions_timeline()\n",
    "\n",
    "# database of current mentions\n",
    "dic_mentions = []\n",
    "for m in mentions:\n",
    "    dic_mentions.append((m.author.screen_name, m.text, m.id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# check mentions for control commands from my Twitter handle\n",
    "# Commands will be preceded by the words \"Heed my command!\"\n",
    "# for example: \"Heed my command! temperature: 0.9\"\n",
    "\n",
    "def check_for_commands(mentions):\n",
    "    for m in mentions:\n",
    "        if (m[0] == \"juandoso\") & (m[1].find(\"Heed my command! \") > -1):\n",
    "            mentions.remove(m)\n",
    "            try:\n",
    "                # Change the diversity factor\n",
    "                if m[1].find(\"temperature:\") > -1:\n",
    "                    i = m[1].find(\"temperature:\")\n",
    "                    TEMPERATURE = float(m[1][i+13:])\n",
    "                    message = \"Changing diversity to {}\".format(TEMPERATURE)\n",
    "                    print(message)\n",
    "                    api.update_status(message)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(\"Command failed\", e)\n",
    "                continue\n",
    "            \n",
    "    return mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Refresh periodically\n",
    "def check_and_tale():\n",
    "    mentions = new_mentions(dic_mentions)\n",
    "    mentions = check_for_commands(mentions)\n",
    "    tales = new_tales(mentions)\n",
    "    respond_with_mentions(tales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# the service\n",
    "import time, threading\n",
    "def running_machine():\n",
    "    #print(\"running...\")\n",
    "    check_and_tale()\n",
    "    threading.Timer(66, running_machine).start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# start\n",
    "print(\"starting microtales machine...\")\n",
    "running_machine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
