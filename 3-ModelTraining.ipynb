{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Generator Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Based on http://nadbordrozd.github.io/blog/2016/09/17/text-generation-with-keras-char-rnns/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, sys, codecs\n",
    "import numpy as np\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we limit ourselves to the following chars.\n",
    "# Uppercase letters will be represented by prefixing them with a U\n",
    "# - a trick proposed by Zygmunt Zajac http://fastml.com/one-weird-trick-for-training-char-rnns/\n",
    "chars = u'\\n !\"#$%&\\'()*+,-./0123456789:;<=>?@[\\\\]abcdefghijklmnopqrstuvwxyzU' + u'\\xa0\\xbb\\xbf\\xc1\\xc9\\xcd\\xd1\\xd3\\xda\\xe0\\xe1\\xe9\\xed\\xf1\\xf3\\xfa\\xfc\\u2015'\n",
    "charset = set(chars)\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "\n",
    "def fix_char(c):\n",
    "    if c.isupper():\n",
    "        return 'U' + c.lower()\n",
    "    elif c in charset:\n",
    "        return c\n",
    "    elif c in [u'\\xa1', u'\\xb0', u'\\u201c', u'\\u201d']:\n",
    "        return '\"'\n",
    "    elif c in [u'\\u2013', u'\\u2014', u'\\u2212', u'\\u2500']:\n",
    "        return '-'\n",
    "    elif c == u'\\u2026':\n",
    "        return '...'\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "\n",
    "def encode(text):\n",
    "    return ''.join(fix_char(c) for c in text)\n",
    "\n",
    "\n",
    "def decode(chars):\n",
    "    upper = False\n",
    "    for c in chars:\n",
    "        if c == 'U':\n",
    "            upper = True\n",
    "        elif upper:\n",
    "            upper = False\n",
    "            yield c.upper()\n",
    "        else:\n",
    "            yield c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_text_slices(path, seqlen=40, step=3):\n",
    "    with codecs.open(path, \"r\", \"utf-8\") as f:\n",
    "        text = f.read().replace('\"','')\n",
    "\n",
    "    # limit the charset, encode uppercase etc\n",
    "    text = encode(text)\n",
    "    yield len(text), text[:seqlen]\n",
    "\n",
    "    while True:\n",
    "        for i in range(0, len(text) - seqlen, step):\n",
    "            sentence = text[i: i + seqlen]\n",
    "            next_char = text[i + seqlen]\n",
    "            yield sentence, next_char\n",
    "\n",
    "\n",
    "def generate_arrays_from_file(path, seqlen=40, step=3, batch_size=10):\n",
    "    slices = generate_text_slices(path, seqlen, step)\n",
    "    text_len, seed = slices.next()\n",
    "    samples = (text_len - seqlen + step - 1)/step\n",
    "    yield samples, seed\n",
    "\n",
    "    while True:\n",
    "        X = np.zeros((batch_size, seqlen, len(chars)), dtype=np.bool)\n",
    "        y = np.zeros((batch_size, len(chars)), dtype=np.bool)\n",
    "        for i in range(batch_size):\n",
    "            sentence, next_char = slices.next()\n",
    "            for t, char in enumerate(sentence):\n",
    "                X[i, t, char_indices[char]] = 1\n",
    "            y[i, char_indices[next_char]] = 1\n",
    "        yield X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(a, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    a = np.log(a) / temperature\n",
    "    a = np.exp(a) / np.sum(np.exp(a))\n",
    "    # this is stupid but np.random.multinomial throws an error if the probabilities\n",
    "    # sum to > 1 - which they do due to finite precision\n",
    "    while sum(a) > 1:\n",
    "        a /= 1.000001\n",
    "    return np.argmax(np.random.multinomial(1, a, 1))\n",
    "\n",
    "\n",
    "def generate(model, seed, diversity):\n",
    "    _, maxlen, _ = model.input_shape\n",
    "    assert len(seed) >= maxlen\n",
    "    sentence = seed[len(seed)-maxlen: len(seed)]\n",
    "    while True:\n",
    "        x = np.zeros((1, maxlen, len(chars)))\n",
    "        for t, char in enumerate(sentence):\n",
    "            x[0, t, char_indices[char]] = 1.\n",
    "\n",
    "        preds = model.predict(x, verbose=0)[0]\n",
    "        next_index = sample(preds, diversity)\n",
    "        next_char = indices_char[next_index]\n",
    "        yield next_char\n",
    "        sentence = sentence[1:] + next_char\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_and_print(model, seed, diversity, n):\n",
    "    sys.stdout.write(' with seed: ')\n",
    "    sys.stdout.write(''.join(decode(seed)))\n",
    "    sys.stdout.write(':\\n')\n",
    "\n",
    "    generator = decode(generate(model, seed, diversity))\n",
    "    sys.stdout.write(''.join(decode(seed)))\n",
    "\n",
    "    full_text = []\n",
    "    for _ in range(n):\n",
    "        next_char = generator.next()\n",
    "        sys.stdout.write(next_char.encode(\"utf-8\"))\n",
    "        sys.stdout.flush()\n",
    "        full_text.append(next_char)\n",
    "\n",
    "    return ''.join(full_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aws_access = {}\n",
    "aws_access[\"Key\"] = \"\"\n",
    "aws_access[\"Secret\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tinys3\n",
    "# save the model definitions and weights to files and optionaly upload them to S3\n",
    "def save_model(model, local_path, bucket, access_data, upload_to_s3=False):\n",
    "    model.save_weights(local_path+'model_weights.h5')\n",
    "    with open(local_path+\"model_definition.yaml\", \"w\") as yaml_file:\n",
    "        yaml_file.write(model.to_yaml())\n",
    "    \n",
    "    if upload_to_s3:\n",
    "        conn = tinys3.Connection(access_data[\"Key\"], access_data[\"Secret\"], tls=True)\n",
    "        f = open(local_path+'model_weights.h5','rb')\n",
    "        conn.upload('model_weights.h5', f, bucket)\n",
    "        f = open(local_path+'model_definition.yaml','rb')\n",
    "        conn.upload('model_definition.yaml', f, bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# main train function\n",
    "def train_lstm(model, input_path, validation_path, save_dir, step=3, batch_size=512, iters=10, save_every=1):\n",
    "    _, seqlen, _ = model.input_shape\n",
    "    train_gen = generate_arrays_from_file(input_path, seqlen=seqlen,\n",
    "                                    step=step, batch_size=batch_size)\n",
    "    samples, seed = train_gen.next()\n",
    "\n",
    "    print 'samples per epoch %s' % samples\n",
    "    last_epoch = model.metadata.get('epoch', 0)\n",
    "\n",
    "    for epoch in range(last_epoch + 1, last_epoch + iters + 1):\n",
    "        val_gen = generate_arrays_from_file(\n",
    "            validation_path, seqlen=seqlen, step=step, batch_size=batch_size)\n",
    "        val_samples, _ = val_gen.next()\n",
    "\n",
    "        hist = model.fit_generator(\n",
    "            train_gen,\n",
    "            validation_data=val_gen,\n",
    "            validation_steps=val_samples/batch_size,\n",
    "            steps_per_epoch=samples/batch_size, epochs=1,\n",
    "            verbose=1)\n",
    "\n",
    "        val_loss = hist.history.get('val_loss', [-1])[0]\n",
    "        loss = hist.history['loss'][0]\n",
    "        model.metadata['loss'].append(loss)\n",
    "        model.metadata['val_loss'].append(val_loss)\n",
    "        model.metadata['epoch'] = epoch\n",
    "\n",
    "        message = 'loss = %.4f   val_loss = %.4f' % (loss, val_loss)\n",
    "        print message\n",
    "        print 'done fitting epoch %s' % epoch\n",
    "        \n",
    "        if epoch % save_every == 0:\n",
    "            print \"saving model\"\n",
    "            save_model(model, save_dir, bucket=\"juandoso-microtales\", access_data=aws_access, upload_to_s3=True)\n",
    "        \n",
    "        print \"generating a sample\" \n",
    "        generate_and_print(model, seed, 0.5, 100)\n",
    "        print \"\\n_____\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#cut the texts into sequences of this many chars\n",
    "maxlen = 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# LSTM model definition\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(maxlen, len(chars)), return_sequences=True))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(LSTM(256, return_sequences=True))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(LSTM(256, return_sequences=False))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(len(chars)))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "model.metadata = {'epoch': 0, 'loss': [], 'val_loss': []}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# training parameters\n",
    "\n",
    "main_path = \"/home/ubuntu/data/\"\n",
    "\n",
    "train_path = main_path + \"tales_2.txt\"\n",
    "test_path = main_path + \"tales_val.txt\"\n",
    "model_dir = main_path\n",
    "step = 1\n",
    "batch_size = 512\n",
    "max_epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples per epoch 224580\n",
      "Epoch 1/1\n",
      "438/438 [==============================] - 302s - loss: 1.2031 - val_loss: 1.3907\n",
      "loss = 1.2031   val_loss = 1.3907\n",
      "done fitting epoch 27\n",
      "generating a sample\n",
      " with seed: \n",
      "Creyó que se repondría, pero lo amaba de verdad.#\n",
      "Con una vida no tenían sufi:\n",
      "\n",
      "Creyó que se repondría, pero lo amaba de verdad.#\n",
      "Con una vida no tenían suficiente.\n",
      "Su propia sombra la sombra de preciosa. Ella era un humano,#\n",
      "No conozco el tiempo.\n",
      "El final,\n",
      "_____\n",
      "Epoch 1/1\n",
      "438/438 [==============================] - 302s - loss: 1.1929 - val_loss: 1.4048\n",
      "loss = 1.1929   val_loss = 1.4048\n",
      "done fitting epoch 28\n",
      "saving model\n",
      "generating a sample\n",
      " with seed: \n",
      "Creyó que se repondría, pero lo amaba de verdad.#\n",
      "Con una vida no tenían sufi:\n",
      "\n",
      "Creyó que se repondría, pero lo amaba de verdad.#\n",
      "Con una vida no tenían suficiente.#\n",
      "Ella se contó en el mar. Y el menos se le dejarás escribir su perro.#\n",
      "Ella te estaba tiene \n",
      "_____\n"
     ]
    }
   ],
   "source": [
    "train_lstm(model=model,\n",
    "           input_path=train_path,\n",
    "           validation_path=test_path,\n",
    "           save_dir=model_dir,\n",
    "           step=step,\n",
    "           batch_size=batch_size,\n",
    "           iters=max_epochs,\n",
    "           save_every=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
